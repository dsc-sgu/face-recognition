{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from facenet import FaceNetNN2\n",
    "from os import listdir\n",
    "from random import choice\n",
    "from typing import Tuple, Generator\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_img = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224,224))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
    "!tar xvf lfw.tgz\n",
    "!rm lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edist(p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Calculates the Euclidean distance between two points in Euclidean space.\n",
    "    The two points are represented by tensors with shape (1, d).\n",
    "    Returns a tensor of one element.\n",
    "    Reference: https://en.wikipedia.org/wiki/Euclidean_distance \n",
    "    '''\n",
    "    return (p - q).pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_triplet(\n",
    "    a_emb: torch.Tensor,\n",
    "    p_emb: torch.Tensor,\n",
    "    n_emb: torch.Tensor,\n",
    "    margin: float\n",
    ") -> bool:\n",
    "    '''\n",
    "    Determines the validity of a triplet for training.\n",
    "    Reference: https://arxiv.org/abs/1503.03832 \n",
    "    '''\n",
    "    if edist(a_emb, p_emb)**2 + margin < edist(a_emb, n_emb)**2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_triplets(\n",
    "    model: torch.nn.Module,\n",
    "    margin: float = 0.0\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray, np.ndarray], None, None]:\n",
    "    '''\n",
    "    Generates valid training triplets from the LFW dataset,\n",
    "        going through all sorts of identities,\n",
    "            but not every image (to speed things up).\n",
    "    '''\n",
    "    identities = listdir('lfw')\n",
    "    for ap_identity in identities:\n",
    "        ap_imgs_name = listdir(f'lfw/{ap_identity}')\n",
    "        identities_without = identities.copy()\n",
    "        identities_without.remove(ap_identity)\n",
    "        for n_identity in identities_without:\n",
    "            n_imgs_name = listdir(f'lfw/{n_identity}')\n",
    "\n",
    "            for n_img_name in n_imgs_name:\n",
    "                a_img_name = choice(ap_imgs_name) if len(ap_imgs_name) > 1 else ap_imgs_name[0]\n",
    "                ap_imgs_name_without = ap_imgs_name.copy()\n",
    "                ap_imgs_name_without.remove(a_img_name)\n",
    "                p_img_name = choice(ap_imgs_name_without) if len(ap_imgs_name) > 1 else a_img_name\n",
    "                a_img = cv2.imread(f'lfw/{ap_identity}/{a_img_name}')\n",
    "                p_img = cv2.imread(f'lfw/{ap_identity}/{p_img_name}') if p_img_name != a_img_name else a_img.copy()\n",
    "                n_img = cv2.imread(f'lfw/{n_identity}/{n_img_name}')\n",
    "                a_img_tensor = preproc_img(a_img).unsqueeze(0)\n",
    "                p_img_tensor = preproc_img(p_img).unsqueeze(0)\n",
    "                n_img_tensor = preproc_img(n_img).unsqueeze(0)\n",
    "                a_emb = model(a_img_tensor)\n",
    "                p_emb = model(p_img_tensor) if p_img_name != a_img_name else a_emb.clone()\n",
    "                n_emb = model(n_img_tensor)\n",
    "\n",
    "                if is_valid_triplet(a_emb, p_emb, n_emb, margin):\n",
    "                    yield (a_img_tensor, p_img_tensor, n_img_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def init_weights(m: torch.nn.Module):\n",
    "    \n",
    "    if getattr(m, 'weight', None) != None:\n",
    "        torch.nn.init.uniform_(m.weight)\n",
    "    if getattr(m, 'bias', None) != None:\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    \n",
    "    for childm in m.children():\n",
    "        init_weights(childm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss():\n",
    "    '''\n",
    "    Triplet Loss implementation.\n",
    "    Reference: https://arxiv.org/abs/1503.03832 \n",
    "    '''\n",
    "    def __init__(self, margin: float):\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, a: torch.Tensor, p: torch.Tensor, n: torch.Tensor) -> torch.Tensor:\n",
    "        return edist(a, p)**2 - edist(a, n)**2 + self.margin        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = FaceNetNN2()\n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_generator = generate_triplets(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "        triplets_generator: Generator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], None, None],\n",
    "        epochs: int\n",
    "    ):\n",
    "\n",
    "    loss_fn = TripletLoss(margin=0.2)\n",
    "    optim_fn = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for tr, triplet in enumerate(triplets_generator):\n",
    "            a_img_tensor, p_img_tensor, n_img_tensor = triplet\n",
    "            optim_fn.zero_grad()\n",
    "            a_emb = model(a_img_tensor)\n",
    "            p_emb = model(p_img_tensor)\n",
    "            n_emb = model(n_img_tensor)\n",
    "            loss = loss_fn(a_emb, p_emb, n_emb)\n",
    "            loss.backward()\n",
    "            optim_fn.step()\n",
    "            \n",
    "            clear_output()\n",
    "            print(f'Epoch: {ep}. Triplet {tr}. Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model,\n",
    "    triplets_generator,\n",
    "    epochs=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
